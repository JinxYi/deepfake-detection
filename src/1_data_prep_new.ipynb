{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf2e4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jinxy\\anaconda3\\envs\\dl-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n",
    "\n",
    "# Import the dataset utilities from your file\n",
    "\n",
    "from utils.dataset_new import load_dataset_and_create_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3DeepfakeDetector(nn.Module):\n",
    "    def __init__(self, pretrained=True, freeze_features=False):\n",
    "        super(MobileNetV3DeepfakeDetector, self).__init__()\n",
    "        \n",
    "        # Load pre-trained MobileNetV3 Large model\n",
    "        if pretrained:\n",
    "            self.model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT)\n",
    "        else:\n",
    "            self.model = mobilenet_v3_large()\n",
    "        \n",
    "        # Freeze feature extraction layers if specified\n",
    "        if freeze_features:\n",
    "            for param in self.model.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier with a custom one for binary classification\n",
    "        in_features = self.model.classifier[0].in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(64, 1)  # Binary classification (fake or real)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e8f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'Early stopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print('Early stopping triggered')\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f858ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    progress_bar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    val_acc = 100 * correct / total\n",
    "    \n",
    "    return val_loss, val_acc, all_preds, all_labels\n",
    "\n",
    "def test(model, loader, criterion, device):\n",
    "    \"\"\"Test the model and calculate various metrics\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store predictions and labels for metrics\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_loss = running_loss / len(loader.dataset)\n",
    "    test_acc = 100 * correct / total\n",
    "    \n",
    "    # Convert to numpy arrays for sklearn\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate Precision-Recall curve and AP\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
    "    ap = average_precision_score(all_labels, all_preds)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    binary_preds = (all_preds > 0.5).astype(int)\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    \n",
    "    results = {\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc,\n",
    "        'roc_auc': roc_auc,\n",
    "        'ap': ap,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_metrics(results, save_path=None):\n",
    "    \"\"\"Plot ROC curve, PR curve, and confusion matrix\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    axes[0].plot(results['fpr'], results['tpr'], color='blue', lw=2,\n",
    "                label=f'ROC curve (AUC = {results[\"roc_auc\"]:.2f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('Receiver Operating Characteristic')\n",
    "    axes[0].legend(loc=\"lower right\")\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    axes[1].plot(results['recall'], results['precision'], color='green', lw=2,\n",
    "                label=f'PR curve (AP = {results[\"ap\"]:.2f})')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].set_title('Precision-Recall Curve')\n",
    "    axes[1].legend(loc=\"lower left\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = results['confusion_matrix']\n",
    "    im = axes[2].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[2].set_title('Confusion Matrix')\n",
    "    tick_marks = np.arange(2)\n",
    "    axes[2].set_xticks(tick_marks)\n",
    "    axes[2].set_yticks(tick_marks)\n",
    "    axes[2].set_xticklabels(['Real', 'Fake'])\n",
    "    axes[2].set_yticklabels(['Real', 'Fake'])\n",
    "    \n",
    "    # Add text annotations to confusion matrix\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[2].text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Metrics plot saved to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a9df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset (use max_samples=5000 for quick development, remove for full dataset)\n",
    "dataset_name = \"xingjunm/WildDeepfake\"\n",
    "\n",
    "# Using the convenience function:\n",
    "# train_loader, val_loader, test_loader = load_dataset_and_create_loaders(\n",
    "#     dataset_name,\n",
    "#     streaming=True,\n",
    "#     max_samples=1000,  # Optional limit\n",
    "#     batch_size=16\n",
    "# )\n",
    "# # Create MobileNetV3-Large model\n",
    "# model = MobileNetDeepfakeDetector(pretrained=True, freeze_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94bf4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trained_model = train_model(\n",
    "#     model,\n",
    "#     train_loader,\n",
    "#     val_loader,\n",
    "#     num_epochs=20,\n",
    "#     learning_rate=0.0001,\n",
    "#     device=device,\n",
    "#     mixed_precision=True  # Use mixed precision for faster training\n",
    "# )\n",
    "\n",
    "# # Save the final model\n",
    "# torch.save(trained_model.state_dict(), 'mobilenet_deepfake_final.pt')\n",
    "\n",
    "# # Evaluate on test set\n",
    "# model.eval()\n",
    "# test_correct = 0\n",
    "# test_total = 0\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for images, labels in test_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "#         outputs = model(images)\n",
    "#         predicted = (torch.sigmoid(outputs.squeeze()) > 0.5).float()\n",
    "#         test_total += labels.size(0)\n",
    "#         test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# test_acc = 100 * test_correct / test_total\n",
    "# print(f'Test Accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1044cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading dataset: xingjunm/WildDeepfake\n",
      "Loading streaming dataset: xingjunm/WildDeepfake\n",
      "Processed 100 examples: 80 train, 10 val, 10 test\n",
      "Processed 200 examples: 160 train, 20 val, 20 test\n",
      "Processed 300 examples: 240 train, 30 val, 30 test\n",
      "Processed 400 examples: 320 train, 40 val, 40 test\n",
      "Processed 500 examples: 400 train, 50 val, 50 test\n",
      "Processed 600 examples: 480 train, 60 val, 60 test\n",
      "Processed 700 examples: 560 train, 70 val, 70 test\n",
      "Processed 800 examples: 640 train, 80 val, 80 test\n",
      "Processed 900 examples: 720 train, 90 val, 90 test\n",
      "Processed 1000 examples: 800 train, 100 val, 100 test\n",
      "Finished processing. Total: 1000\n",
      "Train: 800, Val: 100, Test: 100\n"
     ]
    }
   ],
   "source": [
    "pretrained = True\n",
    "streaming = True\n",
    "max_samples = 1000  # For quick development, remove for full dataset\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-4\n",
    "epochs = 20\n",
    "patience = 5\n",
    "freeze_features = False\n",
    "seed = 42\n",
    "save_path = 'models/mobilenet_deepfake.pth'\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Determine device (GPU or CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset and create loaders\n",
    "print(f\"Loading dataset: {dataset_name}\")\n",
    "train_loader, val_loader, test_loader = load_dataset_and_create_loaders(\n",
    "    dataset_name,\n",
    "    streaming=streaming,\n",
    "    max_samples=max_samples,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42ff5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MobileNetV3 Large model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MobileNetV3DeepfakeDetector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing MobileNetV3 Large model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mMobileNetDeepfakeDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m model = model.to(device)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Define loss function and optimizer\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mMobileNetDeepfakeDetector.__init__\u001b[39m\u001b[34m(self, pretrained, freeze_features)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pretrained=\u001b[38;5;28;01mTrue\u001b[39;00m, freeze_features=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(\u001b[43mMobileNetV3DeepfakeDetector\u001b[49m, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Load pre-trained MobileNetV3 Large model\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n",
      "\u001b[31mNameError\u001b[39m: name 'MobileNetV3DeepfakeDetector' is not defined"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(\"Initializing MobileNetV3 Large model...\")\n",
    "model = MobileNetV3DeepfakeDetector(pretrained=pretrained, freeze_features=freeze_features)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "# Initialize history dictionary\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Initialize best validation loss\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "print(f\"Starting training for {epochs} epochs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    # Check early stopping\n",
    "    if early_stopping(val_loss):\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Print training time\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "# Load best model for testing\n",
    "print(f\"Loading best model from {save_path}\")\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# Test the model\n",
    "print(\"Testing the model...\")\n",
    "test_results = test(model, test_loader, criterion, device)\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Loss: {test_results['test_loss']:.4f}\")\n",
    "print(f\"Test Accuracy: {test_results['test_acc']:.2f}%\")\n",
    "print(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
    "print(f\"Average Precision: {test_results['ap']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plot_metrics(test_results, save_path=save_path.replace('.pth', '_metrics.png'))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_path.replace('.pth', '_history.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
